The paper achieves end effector tracking via integration of afferent encoder and visual signaling. Development uses a IIT iCub humanoid robot simulator, and learns to predict motor signals to achieve "smooth pursuit-like eye movements to track its hand." It does not dive into trajectory planning.

#Proprioceptive
#Robotics 
#Self-Modeling 
#Visuomotor 

#y2018
#HongKongUniversityOfScienceAndTechnology

![[papers/robot_end_effector_tracking_using_predictive_multisensory_integration.pdf]]

## Questions
 - "Eye-hand coordination is a key skill required for these tasks. It requires the integration of multiple sensory modalities, such as vision and proprioception. Human infants appear to learn to develop a sense of themselves through observing the **temporal contingency** and **spatial congruency** of the sensory (e.g., visual, auditory, and proprioceptive)  feedback received during self-produced motion, such as motor babbling (Rochat, 1998)." What is temporal contingency? I assume this means consistency over time, with some relaxation for evolution over time? What is the difference between this and spatial congurency?
 - "Motion in the environment is generated by either self-motion (e.g., of the hand) or exteroceptive motion." Exeroceptive motion is defined as external stimuli (I believe). What does exeroceptive convey that external does not?
 - Development of the primary visual cortex (area V1) depends on visuomotor coupling. What is visuomotor coupling?
 - The "Model Architecture" section first describes the vision component, where an image is captured, and foveal (the region of the eye that captures central vision, responsible for sharp and high resolution capture) subwindows are constructed, one fine-scale and central, the other course and subsampled. This seems like something that currently is achieved inherently by CNN's, a network modality originally conceived well before 2015.

## Notes

 - Neurons in the primary visual cortex (area V1) are driven not only by visual but also by the motor input (at least in mice). Development of this area depends on visuomotor coupling.
 - Model input is imagery plus proprioceptive measuremments via joint encoders. "Produces eye motor actions to track the moving robot hand."
 - "First, the learning does not depend on any fiducial visual marker to identify the end effector of the robot. Second, the model does not require the forward kinematics of the arm to be known. Third, pre-defined visual feature descriptors are not required, but rather are learned"
 - Autonomous learning of a body schema (a sensorimotor representation of the body) encapsulates learning end effector tracking
 - Hoffmann classifies body representations as either implicit or explicit. In explicit, sensory and motor coordinates are broken into chains of individual transformations, with each link matching a particular piece of the robot structure. Implicit models clump all of the representation into one big mess, using look-up tables or neural models.
 - **Efficient Coding Hypothesis** - A theoretical model of sensory coding in the brain, in which the number of electrical spikes necessary to transmit information is minimized. In particular, Horace Barlow (1961) believed that this was achieved with a type of "coding" near-optimal for audio and visual information and **tuned for our environment**.
 - **Active Efficient Coding (AEC)** - An application of the theory behind the Efficient Coding Hypothesis that also learns movements of the sensory organs, to improve compression. In other words, "visual, and proprioceptive stimuli are jointly encoded."
 - From ASSOM paper, "The initial stages of the visual cortex perform a feature extraction task"
 - "The concept of **invariant feature detectors** is closely related to the concept of subspaces or **manifolds** of the input space. Natural inputs, such as auditory or visual signals, are usually high dimensional, but distributed along many lower dimensional manifolds. For example, although objects in the visual field may be quite complex, neurons in the initial stages of visual processing respond to relatively small spatial regions. At this scale, the typical inputs may be relatively simple and regular (e.g. oriented edges), but vary according to a class of transformations, such as translation. The inputs corresponding to one common pattern \[convolutional filter?] subject to different transformations within this class are referred to as an **invariance class**, and lie along a manifold of the high dimensional input space. If the transformation is a linear operation, then the manifold is a linear subspace. Invariant features correspond to subspaces, with the strength of each feature depending upon the length of the projection of the input onto the subspace." 

## Anatomy-Related Definitions
 - foveal - the region of the eye that captures central vision, responsible for sharp and high resolution capture
 - cortical - outer layer of the cerebrum
 - cerebrum - anterior region of the brain in vertebrates responsible for coordination of voluntary activity in the body. It is broken up into two hemispheres, separated by a fissure. Despite the official definitions using anterior, it is the topmost section which spans the foremost and hindmost locations of the brain, and is moreso on top and spanning far more forward than the cerebellum
 - cerebral mantle - also known as the cerebral cortex, is the outer layer of the neural tissue of the cerebrum
 - cortex - outermost layer of an organ or plant
 - epithelium - thin tissue forming the outer layer of the body's surface. The topmost layer is the epidermis. This can refer to plants or animals.
 - Visual cortex - most literally, the outermost layer of the visual sector of the brain, it is primarily located in the occipital lobe of the primary cerebral cortex, in the most posterior region of the brain. The eyes connect to the occipital lobe through the optic nerve. Visual cortical cells are loosely categorized as simple or complex - both of which exhibit orientation and spatial frequency selectivity. However, simple cells are highly sensitive to phase (position), while complex cells are invariant, and assumed to be inputs to higher cortical stages.
 - Efferent signals travel outward from something (brain, for nerve signaling, or heart, for blood vessels). Afferent signals travel inward toward something.

## LinAlg / ML Definitions
 - **Generative Adaptive Subspace Self Organizing Maps (GASSOMs)** ([Chandrapala and Shi, 2015](https://openreview.net/forum?id=EN4iRdhDef)) [pdf-link](http://www.cmap.polytechnique.fr/~nikolaus.hansen/proceedings/2014/WCCI/IJCNN-2014/PROGRAM/N-14673.pdf). An **Adaptive Subspace Self Organizing Map (ASSOM)** is made up of a two dimensional latent space of nodes which each represent a single invariant feature detector - i.e. a singe feature (filter/vector?) over all linear transformations. (LOOSELY) During training, each input vector is projected onto the basis vectors of all nodes, and the node with the highest squared projection length "wins" the input. This winning node, along with its neighbors, adjusts its basis vectors to better represent the input, leading to a clustering effect of features within the map. The generative extension in GASSOM addresses a key limitation of ASSOM: the need for explicitly defined temporal episodes during training. Instead of relying on pre-defined episode boundaries, GASSOM employs a probabilistic model where nodes generate input vectors based on a hidden Markov process, capturing temporal continuity inherently.
 - A subspace is a subset of a vector space (also known as a linear space - set whose elements or vectors can be added and multiplied by scalars, plus the vector axioms) that is also a vector space under the same operations.
 - A manifold is a topological space (geometrical space in which closeness is defined) that locally looks like a real n-dimensional Euclidean vector space.

## Technologies

 - iCub humanoid robot simulator -  A research-grade humanoid robot designed to test embodied AI algo's. Developed by Istituto Italiano Di Tecnologia (IIT), iCub is a minature robot replicating a humanoid robots basic functions, including actuators to enable manipulation, vision, and locomotion.
 


## Methodology

1. An image is captured from the iCub's low-res right eye
2. Course and fine-scale foveal subwindows are extracted 
3. Visual stimuli are encoded using GASSOMs
4. Visual and proprioceptive (encoder PVA) inputs are passed to a "prediction module", which predicts sensory consequences of arm and eye movements.
5. Visual and proprioceptive inputs are integrated in an ANN to generate pan and tilt eye acceleration commands enabling the eye to track the robot end effector.
6. The model only uses afferent signals, and therefore the control of the robot arm is left to an operator, motor babbling, or a predefined motion pattern.
7. The prediction is discrete time, and after actuation of the eye, jump to step 1.





## Reference Images

![[Pasted image 20250105131642.png]]


## ToDo
 - Learn / scribe the basic sizing (similar to the [Latency Numbers all Computer Scientists Should Know](https://gist.github.com/hellerbarde/2843375)) of neurons. I.e. how wide is a nerve cell compared to a human hair, variety of length of nerve cells, how does the nerve cell width vary with distance of the cell, why do squids have enormous nerve cells?